{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/goodreads_reviews_spoiler.json') as json_file:\n",
    "    data = json.loads('[' + ',\\n'.join(json_file.readlines()) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(sents):\n",
    "    ys = []\n",
    "    xs = []\n",
    "    for y, sent in sents:\n",
    "        ys.append(y)\n",
    "        xs.append(sent)\n",
    "    return ys, xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1378033/1378033 [00:04<00:00, 342278.32it/s]\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "for sample in tqdm(data):\n",
    "    ans, sent = unpack(sample['review_sentences'])\n",
    "    x.extend(sent)\n",
    "    y.extend(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17672655/17672655 [00:34<00:00, 513027.07it/s]\n"
     ]
    }
   ],
   "source": [
    "for id, i in tqdm(enumerate(x), total=len(x)):\n",
    "    text = re.sub(r'http\\S+', '', i.lower())\n",
    "    # text = str(TextBlob(text).correct())\n",
    "    x[id] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_data = pd.DataFrame([x,y], index=['text', 'target']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_data = prep_data[~(prep_data['text'] == '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(prep_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('dataset/baseline/train.csv', index=False)\n",
    "test.to_csv('dataset/baseline/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = 'config/baseline_config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text-clf-train - INFO - Config:\n",
      "\n",
      "seed: 42\n",
      "path_to_save_folder: models\n",
      "experiment_name: model\n",
      "\n",
      "# data\n",
      "data:\n",
      "  train_data_path: dataset/baseline/train.csv\n",
      "  test_data_path: dataset/baseline/test.csv\n",
      "  sep: ','\n",
      "  text_column: text\n",
      "  target_column: target\n",
      "\n",
      "# preprocessing\n",
      "# (included in resulting model pipeline, so preserved for inference)\n",
      "preprocessing:\n",
      "  lemmatization: null  # pymorphy2\n",
      "\n",
      "# tf-idf\n",
      "tf-idf:\n",
      "  lowercase: true\n",
      "  ngram_range: (1, 1)\n",
      "  max_df: 1.0\n",
      "  min_df: 1\n",
      "\n",
      "# logreg\n",
      "logreg:\n",
      "  penalty: l2\n",
      "  C: 1.0\n",
      "  class_weight: balanced\n",
      "  solver: saga\n",
      "  n_jobs: -1\n",
      "\n",
      "# grid-search\n",
      "grid-search:\n",
      "  do_grid_search: false\n",
      "  grid_search_params_path: hyperparams.py\n",
      "text-clf-train - INFO - Loading data...\n",
      "text-clf-train - INFO - Train dataset size: 14127218\n",
      "text-clf-train - INFO - Test dataset size: 3531805\n",
      "text-clf-train - INFO - Fitting TF-IDF + LogReg model...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/xx/anaconda3/envs/diploma/lib/python3.8/site-packages/text_clf/__main__.py\", line 33, in train\n",
      "    pipe, target_names_mapping = _train(\n",
      "  File \"/home/xx/anaconda3/envs/diploma/lib/python3.8/site-packages/text_clf/train.py\", line 91, in _train\n",
      "    pipe.fit(X_train, y_train)\n",
      "  File \"/home/xx/anaconda3/envs/diploma/lib/python3.8/site-packages/sklearn/pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/home/xx/anaconda3/envs/diploma/lib/python3.8/site-packages/sklearn/pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/home/xx/anaconda3/envs/diploma/lib/python3.8/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/home/xx/anaconda3/envs/diploma/lib/python3.8/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/home/xx/anaconda3/envs/diploma/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 2077, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/home/xx/anaconda3/envs/diploma/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1330, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"/home/xx/anaconda3/envs/diploma/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/home/xx/anaconda3/envs/diploma/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 108, in _analyze\n",
      "    doc = decoder(doc)\n",
      "  File \"/home/xx/anaconda3/envs/diploma/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 226, in decode\n",
      "    raise ValueError(\n",
      "ValueError: np.nan is an invalid document, expected byte or unicode string.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, target_names_mapping = text_clf.train(path_to_config=CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd8a5574eb3498354326dd8f7f5637662e328bd61c3c4a6f6aab62d7a89b6797"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('tf2_gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "dd8a5574eb3498354326dd8f7f5637662e328bd61c3c4a6f6aab62d7a89b6797"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
